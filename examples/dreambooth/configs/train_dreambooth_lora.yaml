# Path to pretrained model or model identifier from huggingface.co/models
pretrained_model_name_or_path: CompVis/stable-diffusion-v1-4

# Revision of pretrained model identifier from huggingface.co/models
revision: null 

# Pretrained tokenizer name or path if not the same as model_name
tokenizer_name: null

# A folder containing the training data of instance images
instance_data_dir: INSTANCE_DIR

# A folder containing the training data of class images
class_data_dir: null

# The prompt with identifier specifying the instance
instance_prompt: A photo of sks dog

# The prompt to specify images in the same class as provided instance images
class_prompt: null

# A prompt that is used during validation to verify that the model is learning
validation_prompt: null

# Number of images that should be generated during validation with `validation_prompt`
num_validation_images: 4

# Run dreambooth validation every X epochs
validation_epochs: 50

# Flag to add prior preservation loss
with_prior_preservation: false

# The weight of prior preservation loss
prior_loss_weight: 1.0

# Minimal class images for prior preservation loss
num_class_images: 100

# Output and Seed
# The output directory where the model predictions and checkpoints will be written
output_dir: "lora-dreambooth-model"

# A seed for reproducible training
seed: null

# Image Processing
# The resolution for input images
resolution: 512

# Whether to center crop the input images to the resolution
center_crop: false

# Training Configuration
# Whether to train the text encoder
train_text_encoder: false

# Batch size (per device) for the training dataloader
train_batch_size: 4

# Batch size (per device) for sampling images
sample_batch_size: 4

# The number of epochs to train
num_train_epochs: 1

# Total number of training steps to perform
max_train_steps: null

# Save a checkpoint of the training state every X updates
checkpointing_steps: 500

# Max number of checkpoints to store
checkpoints_total_limit: null

# Whether training should be resumed from a previous checkpoint
resume_from_checkpoint: null

# Number of updates steps to accumulate before performing a backward/update pass
gradient_accumulation_steps: 1

# Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass
gradient_checkpointing: false

# Initial learning rate (after the potential warmup period) to use
learning_rate: 0.0005

# Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size
scale_lr: false

# The scheduler type to use
lr_scheduler: "constant"

# Number of steps for the warmup in the lr scheduler
lr_warmup_steps: 500

# Number of hard resets of the lr in cosine_with_restarts scheduler
lr_num_cycles: 1

# Power factor of the polynomial scheduler
lr_power: 1.0

# Number of subprocesses to use for data loading
dataloader_num_workers: 0

# Optimizer Configuration
# Whether or not to use 8-bit Adam from bitsandbytes
use_8bit_adam: false

# The beta1 parameter for the Adam optimizer
adam_beta1: 0.9

# The beta2 parameter for the Adam optimizer
adam_beta2: 0.999

# Weight decay to use
adam_weight_decay: 0.01

# Epsilon value for the Adam optimizer
adam_epsilon: 0.00000001

# Max gradient norm
max_grad_norm: 1.0

# Hub and Logging Configuration
# Whether or not to push the model to the Hub
push_to_hub: false

# The token to use to push to the Model Hub
hub_token: null

# The name of the repository to keep in sync with the local `output_dir`
hub_model_id: null

# TensorBoard log directory
logging_dir: "logs"

# Miscellaneous Configuration
# Whether or not to allow TF32 on Ampere GPUs
allow_tf32: false

# The integration to report the results and logs to
report_to: "tensorboard"

# Whether to use mixed precision
mixed_precision: null

# Choose prior generation precision
prior_generation_precision: null

# For distributed training: local_rank
local_rank: -1

# Whether or not to use xformers
enable_xformers_memory_efficient_attention: false

# Whether or not to pre-compute text embeddings
pre_compute_text_embeddings: false

# The maximum length of the tokenizer
tokenizer_max_length: null

# Whether to use attention mask for the text encoder
text_encoder_use_attention_mask: false

# Optional set of images to use for validation
validation_images: null

# The optional `class_label` conditioning to pass to the unet
class_labels_conditioning: null

# The dimension of the LoRA update matrices
rank: 4