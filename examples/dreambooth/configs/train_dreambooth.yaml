# Path to pretrained model or model identifier from huggingface.co/models
pretrained_model_name_or_path: CompVis/stable-diffusion-v1-4

# Revision of pretrained model identifier from huggingface.co/models. Trainable model components should be float32 precision
revision: null

# Pretrained tokenizer name or path if not the same as model_name
tokenizer_name: null

# A folder containing the training data of instance images
instance_data_dir: INSTANCE_DIR

# A folder containing the training data of class images
class_data_dir: null

# The prompt with identifier specifying the instance
instance_prompt: A photo of sks dog

# The prompt to specify images in the same class as provided instance images
class_prompt: null

# Flag to add prior preservation loss
with_prior_preservation: false

# The weight of prior preservation loss
prior_loss_weight: 1.0

# Minimal class images for prior preservation loss. If there are not enough images already present in class_data_dir, additional images will be sampled with class_prompt
num_class_images: 100

# The output directory where the model predictions and checkpoints will be written
output_dir: "text-inversion-model"

# A seed for reproducible training
seed: null

# The resolution for input images, all the images in the train/validation dataset will be resized to this resolution
resolution: 512

# Whether to center crop the input images to the resolution. If not set, the images will be randomly cropped. The images will be resized to the resolution first before cropping
center_crop: false

# Whether to train the text encoder. If set, the text encoder should be float32 precision
train_text_encoder: false

# Batch size (per device) for the training dataloader
train_batch_size: 4

# Batch size (per device) for sampling images
sample_batch_size: 4

# The number of epochs to train
num_train_epochs: 1

# Total number of training steps to perform.  If provided, overrides num_train_epochs
max_train_steps: null

# Save a checkpoint of the training state every X updates. Checkpoints can be used for resuming training via `--resume_from_checkpoint`
checkpointing_steps: 500

# Max number of checkpoints to store. Passed as `total_limit` to the `Accelerator` `ProjectConfiguration`
checkpoints_total_limit: null

# Whether training should be resumed from a previous checkpoint. Use a path saved by `--checkpointing_steps`, or `"latest"` to automatically select the last available checkpoint
resume_from_checkpoint: null

# Number of updates steps to accumulate before performing a backward/update pass
gradient_accumulation_steps: 1

# Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass
gradient_checkpointing: false

# Initial learning rate (after the potential warmup period) to use
learning_rate: 0.000005

# Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size
scale_lr: false

# The scheduler type to use. Choose between ["linear", "cosine", "cosine_with_restarts", "polynomial", "constant", "constant_with_warmup"]
lr_scheduler: "constant"

# Number of steps for the warmup in the lr scheduler
lr_warmup_steps: 500

# Number of hard resets of the lr in cosine_with_restarts scheduler
lr_num_cycles: 1

# Power factor of the polynomial scheduler
lr_power: 1.0

# Whether or not to use 8-bit Adam from bitsandbytes
use_8bit_adam: false

# Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process
dataloader_num_workers: 0

# The beta1 parameter for the Adam optimizer
adam_beta1: 0.9

# The beta2 parameter for the Adam optimizer
adam_beta2: 0.999

# Weight decay to use
adam_weight_decay: 0.01

# Epsilon value for the Adam optimizer
adam_epsilon: 0.00000001

# Max gradient norm
max_grad_norm: 1.0

# Whether or not to push the model to the Hub
push_to_hub: false

# The token to use to push to the Model Hub
hub_token: null

# The name of the repository to keep in sync with the local `output_dir`
hub_model_id: null

# [TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***
logging_dir: "logs"

# Whether or not to allow TF32 on Ampere GPUs. Can be used to speed up training. For more information, see https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices
allow_tf32: false

# The integration to report the results and logs to. Supported platforms are `"tensorboard"` (default), `"wandb"` and `"comet_ml"`. Use `"all"` to report to all integrations
report_to: "tensorboard"

# A prompt that is used during validation to verify that the model is learning
validation_prompt: null

# Number of images that should be generated during validation with `validation_prompt`
num_validation_images: 4

# Run validation every X steps. Validation consists of running the prompt `args.validation_prompt` multiple times: `args.num_validation_images` and logging the images
validation_steps: 100

# Whether to use mixed precision. Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >= 1.10.and an Nvidia Ampere GPU.  Default to the value of accelerate config of the current system or the flag passed with the `accelerate.launch` command. Use this argument to override the accelerate config
mixed_precision: null

# Choose prior generation precision between fp32, fp16 and bf16 (bfloat16). Bf16 requires PyTorch >= 1.10.and an Nvidia Ampere GPU.  Default to  fp16 if a GPU is available else fp32
prior_generation_precision: null

# For distributed training: local_rank
local_rank: -1

# Whether or not to use xformers
enable_xformers_memory_efficient_attention: false

# Save more memory by using setting grads to None instead of zero. Be aware, that this changes certain behaviors, so disable this argument if it causes any problems. More info: https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html
set_grads_to_none: false

# Fine-tuning against a modified noise. See: https://www.crosslabs.org//blog/diffusion-with-offset-noise for more information
offset_noise: false

# Whether or not to pre-compute text embeddings. If text embeddings are pre-computed, the text encoder will not be kept in memory during training and will leave more GPU memory available for training the rest of the model. This is not compatible with `--train_text_encoder`
pre_compute_text_embeddings: false

# The maximum length of the tokenizer. If not set, will default to the tokenizer's max length
tokenizer_max_length: null

# Whether to use attention mask for the text encoder
text_encoder_use_attention_mask: false

# Set to not save text encoder
skip_save_text_encoder: false

# Optional set of images to use for validation. Used when the target pipeline takes an initial image as input such as when training image variation or superresolution
validation_images: null

# The optional `class_label` conditioning to pass to the unet, available values are `timesteps`
class_labels_conditioning: null